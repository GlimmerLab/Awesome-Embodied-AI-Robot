import os
import re
import json
import requests
import arxiv
from datetime import datetime
from bs4 import BeautifulSoup
from tqdm import tqdm
import string

class PaperUpdater:
    def __init__(self):
        self.base_url = "https://arxiv.paperswithcode.com/api/v0/papers/"
        # å®šä¹‰ç›®å½•å’Œå¯¹åº”çš„æ£€ç´¢å…³é”®è¯
        self.categories = {
            "Motion-Planning": "motion planning OR trajectory optimization OR path planning",
            "Task-Planning": "task planning OR task decomposition OR task scheduling",
            "Simulation-Platforms": "simulation platform OR physics engine OR robot simulation",
            "Robot-Learning-and-Reinforcement-Learning": "reinforcement learning OR RL OR policy learning OR robot learning OR legged robot OR quadruped OR biped OR locomotion OR motor skill OR imitation learning OR policy gradient OR PPO OR SAC OR DDPG",
            "Multimodal-Interaction": "multimodal OR multi-modal OR cross-modal OR human-robot interaction OR vision language OR visual language",
            "Environment-Perception": "environment perception OR scene understanding OR object detection OR visual perception",
            "Fundamental-Theory": "embodied AI OR embodied intelligence OR embodied learning OR cognitive science OR control theory"
        }

        # åˆå§‹åŒ–æ‰‹åŠ¨æ·»åŠ çš„è®ºæ–‡å­—å…¸
        self.manually_added_papers = {}

        # å®šä¹‰å„ä¸»é¢˜çš„æ ‡ç­¾å’Œå…³é”®è¯
        self.tags = {
            # å¼ºåŒ–å­¦ä¹ æ ‡ç­¾
            "Robot-Learning-and-Reinforcement-Learning": {
                "Foundational": ["foundational", "fundamental", "core", "basic", "essential"],
                "Imitation": ["imitation", "demonstration", "expert", "teacher", "learning from demonstration"],
                "Milestone": ["milestone", "breakthrough", "significant", "important", "key"],
                "Biped": ["biped", "bipedal", "humanoid", "two-legged"],
                "Quadruped": ["quadruped", "four-legged", "dog", "animal"],
                "AMP": ["AMP", "adversarial", "motion prior", "style", "character"],
                "Adaptation": ["adaptation", "transfer", "generalization", "robust", "resilient"],
                "Policy": ["policy", "actor-critic", "PPO", "SAC", "DDPG", "TRPO"],
                "Sim2Real": ["sim2real", "sim-to-real", "transfer", "domain", "reality gap"],
                "Hierarchical": ["hierarchical", "HRL", "option", "skill", "subgoal"],
                "Multi-Task": ["multi-task", "multi-task learning", "generalist", "versatile"],
                "Offline": ["offline", "batch", "off-policy", "data-driven"],
                "Meta": ["meta", "meta-learning", "few-shot", "adaptation"],
                "Multi-Agent": ["multi-agent", "collaboration", "cooperation", "interaction"]
            },
            # è¿åŠ¨è§„åˆ’æ ‡ç­¾
            "Motion-Planning": {
                "Trajectory": ["trajectory", "path", "motion", "planning", "optimization"],
                "Collision": ["collision", "avoidance", "safety", "obstacle"],
                "Real-time": ["real-time", "realtime", "fast", "efficient", "computational"],
                "Learning": ["learning", "learned", "neural", "deep", "ML"],
                "Dynamic": ["dynamic", "dynamics", "kinematic", "kinematics"],
                "Uncertainty": ["uncertainty", "robust", "stochastic", "probabilistic"],
                "Multi-robot": ["multi-robot", "swarm", "collaborative", "coordination"],
                "Reactive": ["reactive", "reaction", "responsive", "adaptive"]
            },
            # ä»»åŠ¡è§„åˆ’æ ‡ç­¾
            "Task-Planning": {
                "Hierarchical": ["hierarchical", "hierarchy", "decomposition", "subtask"],
                "Temporal": ["temporal", "temporal logic", "sequence", "ordering"],
                "Learning": ["learning", "learned", "neural", "deep", "ML"],
                "Semantic": ["semantic", "language", "instruction", "command"],
                "Multi-agent": ["multi-agent", "collaboration", "cooperation", "interaction"],
                "Uncertainty": ["uncertainty", "robust", "stochastic", "probabilistic"],
                "Interactive": ["interactive", "human", "user", "feedback"],
                "Long-horizon": ["long-horizon", "long-term", "complex", "sequential"]
            },
            # ä»¿çœŸå¹³å°æ ‡ç­¾
            "Simulation-Platforms": {
                "Physics": ["physics", "engine", "simulation", "dynamics"],
                "Visual": ["visual", "rendering", "graphics", "3D"],
                "Realistic": ["realistic", "realism", "photo-realistic", "high-fidelity"],
                "Benchmark": ["benchmark", "evaluation", "test", "challenge"],
                "Multi-domain": ["multi-domain", "multi-task", "versatile", "general"],
                "Open-source": ["open-source", "open source", "community", "collaborative"],
                "Scalable": ["scalable", "parallel", "distributed", "efficient"],
                "Interactive": ["interactive", "user", "human", "interface"]
            },
            # å¤šæ¨¡æ€äº¤äº’æ ‡ç­¾
            "Multimodal-Interaction": {
                "Vision-Language": ["vision-language", "visual language", "image-text", "multimodal"],
                "Human-Robot": ["human-robot", "human robot", "interaction", "collaboration"],
                "Gesture": ["gesture", "motion", "body", "sign"],
                "Speech": ["speech", "audio", "voice", "sound"],
                "Tactile": ["tactile", "touch", "haptic", "force"],
                "Learning": ["learning", "learned", "neural", "deep", "ML"],
                "Real-time": ["real-time", "realtime", "fast", "efficient"],
                "Social": ["social", "emotional", "cognitive", "mental"]
            },
            # ç¯å¢ƒæ„ŸçŸ¥æ ‡ç­¾
            "Environment-Perception": {
                "Object": ["object", "detection", "recognition", "tracking"],
                "Scene": ["scene", "understanding", "parsing", "segmentation"],
                "3D": ["3D", "point cloud", "depth", "stereo"],
                "Semantic": ["semantic", "meaning", "interpretation", "reasoning"],
                "Learning": ["learning", "learned", "neural", "deep", "ML"],
                "Real-time": ["real-time", "realtime", "fast", "efficient"],
                "Robust": ["robust", "robustness", "adversarial", "attack"],
                "Multi-modal": ["multi-modal", "multimodal", "fusion", "integration"]
            },
            # åŸºç¡€ç†è®ºæ ‡ç­¾
            "Fundamental-Theory": {
                "Cognitive": ["cognitive", "cognition", "mental", "brain"],
                "Control": ["control", "controller", "stability", "dynamics"],
                "Learning": ["learning", "learned", "neural", "deep", "ML"],
                "Representation": ["representation", "embedding", "feature", "latent"],
                "Generalization": ["generalization", "transfer", "adaptation", "robust"],
                "Theory": ["theory", "theoretical", "analysis", "proof"],
                "Survey": ["survey", "review", "overview", "tutorial"],
                "Benchmark": ["benchmark", "evaluation", "test", "challenge"]
            }
        }

        self.existing_papers = self.load_existing_papers()
        self.keywords = self.extract_keywords_from_existing_papers()

    def ensure_directory_exists(self, directory):
        """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"åˆ›å»ºç›®å½•: {directory}")

    def load_existing_papers(self):
        """åŠ è½½ç°æœ‰è®ºæ–‡ä¿¡æ¯"""
        existing_papers = {}
        for category in self.categories.keys():
            existing_papers[category] = []
            en_file = f"{category}/README.md"
            cn_file = f"{category}/README_CN.md"

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            self.ensure_directory_exists(category)

            if os.path.exists(en_file):
                with open(en_file, "r", encoding="utf-8") as f:
                    content = f.read()

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ‰‹åŠ¨æ·»åŠ çš„è®ºæ–‡å—
                    manual_papers_match = re.search(r'## Manually Added Papers\n\n(.*?)(?=\n## |$)', content, re.DOTALL)
                    if manual_papers_match:
                        manual_papers_content = manual_papers_match.group(1)
                        # æå–æ‰‹åŠ¨æ·»åŠ çš„è®ºæ–‡ä¿¡æ¯
                        manual_table_rows = re.findall(r'\|(.*?)\|(.*?)\|(.*?)\|(.*?)\|(.*?)\|', manual_papers_content)
                        for row in manual_table_rows:
                            # è·³è¿‡è¡¨å¤´è¡Œ
                            if row[0].strip() == "Date" or row[0].strip() == "æ—¥æœŸ":
                                continue
                            if row[0].strip() == ":---:":
                                continue

                            date, title, pdf_link, code_link, rating = row
                            # æå–PDFé“¾æ¥
                            pdf_url_match = re.search(r'\[\[pdf\]\]\((.*?)\)', pdf_link)
                            pdf_url = pdf_url_match.group(1) if pdf_url_match else pdf_link

                            # æå–ä»£ç é“¾æ¥
                            code_url_match = re.search(r'\[(.*?)\]\((.*?)\)', code_link)
                            if code_url_match:
                                code_url = code_url_match.group(2)
                            else:
                                code_url = code_url_match.group(0) if code_url_match else code_link.strip()

                            # æ¸…ç†æ ‡é¢˜
                            clean_title = title.strip()

                            paper_info = {
                                "date": date.strip(),
                                "title": clean_title,
                                "pdf_url": pdf_url,
                                "code_url": code_url,
                                "rating": rating.strip(),
                                "manual": True  # æ ‡è®°ä¸ºæ‰‹åŠ¨æ·»åŠ 
                            }
                            existing_papers[category].append(paper_info)

                            # å°†æ‰‹åŠ¨æ·»åŠ çš„è®ºæ–‡ä¿å­˜åˆ°manually_added_paperså­—å…¸ä¸­
                            if category not in self.manually_added_papers:
                                self.manually_added_papers[category] = []
                            self.manually_added_papers[category].append(paper_info)

                    # æå–è‡ªåŠ¨æ›´æ–°çš„è®ºæ–‡ä¿¡æ¯
                    auto_papers_match = re.search(r'## Auto-Updated Papers\n\n(.*?)(?=\n## |$)', content, re.DOTALL)
                    if auto_papers_match:
                        auto_papers_content = auto_papers_match.group(1)
                        table_rows = re.findall(r'\|(.*?)\|(.*?)\|(.*?)\|(.*?)\|(.*?)\|', auto_papers_content)
                        for row in table_rows:
                            # è·³è¿‡è¡¨å¤´è¡Œ
                            if row[0].strip() == "Date" or row[0].strip() == "æ—¥æœŸ":
                                continue
                            if row[0].strip() == ":---:":
                                continue

                            date, title, pdf_link, code_link, rating = row
                            # æå–PDFé“¾æ¥
                            pdf_url_match = re.search(r'\[\[pdf\]\]\((.*?)\)', pdf_link)
                            pdf_url = pdf_url_match.group(1) if pdf_url_match else pdf_link

                            # æå–ä»£ç é“¾æ¥
                            code_url_match = re.search(r'\[(.*?)\]\((.*?)\)', code_link)
                            if code_url_match:
                                code_url = code_url_match.group(2)
                            else:
                                code_url = code_url_match.group(0) if code_url_match else code_link.strip()

                            # æ¸…ç†æ ‡é¢˜
                            clean_title = title.strip()

                            existing_papers[category].append({
                                "date": date.strip(),
                                "title": clean_title,
                                "pdf_url": pdf_url,
                                "code_url": code_url,
                                "rating": rating.strip(),
                                "manual": False  # æ ‡è®°ä¸ºè‡ªåŠ¨æ›´æ–°
                            })

        return existing_papers

    def extract_keywords_from_existing_papers(self):
        """ä»ç°æœ‰è®ºæ–‡æ ‡é¢˜ä¸­æå–å…³é”®è¯"""
        keywords = {}
        for category, papers in self.existing_papers.items():
            if not papers:
                keywords[category] = self.categories[category]
                continue

            # æ”¶é›†æ‰€æœ‰æ ‡é¢˜
            titles = [paper["title"] for paper in papers]

            # ç®€å•åˆ†è¯å¹¶æå–å…³é”®è¯
            all_words = []
            for title in titles:
                # ç®€å•çš„åˆ†è¯æ–¹æ³•ï¼ŒæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·åˆ†å‰²
                words = re.findall(r'\b\w+\b', title.lower())
                # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¯
                words = [word for word in words if len(word) > 3]
                all_words.extend(words)

            # ç»Ÿè®¡è¯é¢‘
            word_freq = {}
            for word in all_words:
                if word in word_freq:
                    word_freq[word] += 1
                else:
                    word_freq[word] = 1

            # é€‰æ‹©æœ€å¸¸è§çš„è¯ä½œä¸ºå…³é”®è¯
            top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
            keyword_query = " OR ".join([word for word, _ in top_words])

            # ç»“åˆåŸå§‹æŸ¥è¯¢å’Œæå–çš„å…³é”®è¯
            keywords[category] = f"({self.categories[category]}) AND ({keyword_query})"

        return keywords

    def get_paper_info(self, paper_id):
        """è·å–è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä»£ç é“¾æ¥"""
        try:
            code_url = self.base_url + paper_id
            response = requests.get(code_url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if "official" in data and "url" in data["official"]:
                    return data["official"]["url"]
            return None
        except:
            return None

    def identify_tag(self, category, title, abstract):
        """è¯†åˆ«è®ºæ–‡çš„æ ‡ç­¾"""
        # ä¸å†ä½¿ç”¨æ ‡ç­¾ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        return ""

    def get_daily_papers(self, category, query, max_results=1):
        """è·å–æ¯æ—¥è®ºæ–‡"""
        papers = []
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )

        for result in tqdm(search.results(), desc=f"è·å–{category}è®ºæ–‡"):
            paper_id = result.get_short_id()

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if any(paper["title"] == result.title for paper in self.existing_papers[category]):
                continue

            code_url = self.get_paper_info(paper_id)

            # å°è¯•è¯†åˆ«æ ‡ç­¾
            title = result.title
            tag = self.identify_tag(category, title, result.summary)

            # å¦‚æœæ ‡é¢˜ä¸­æœ‰å†’å·ï¼Œå°è¯•æå–æ ‡ç­¾
            if not tag and ":" in title:
                tag = title.split(":")[0].strip()
                title = title.split(":")[1].strip()

            paper_info = {
                "date": str(result.published.date()),
                "title": title,
                "tag": tag,
                "authors": [author.name for author in result.authors],
                "abstract": result.summary,
                "pdf_url": result.entry_id,
                "code_url": code_url if code_url else "âš ï¸",
                "rating": "â­ï¸â­ï¸â­ï¸",  # é»˜è®¤è¯„åˆ†
                "has_code": code_url is not None,
                "manual": False  # æ ‡è®°ä¸ºè‡ªåŠ¨æ›´æ–°
            }
            papers.append(paper_info)

        return papers

    def update_category_readme(self, category, new_papers):
        """æ›´æ–°åˆ†ç±»ç›®å½•çš„READMEæ–‡ä»¶"""
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.ensure_directory_exists(category)

        # è·å–ç°æœ‰çš„æ‰‹åŠ¨æ·»åŠ è®ºæ–‡
        existing_manual_papers = self.manually_added_papers.get(category, [])

        # è¿‡æ»¤æ‰å·²ç»å­˜åœ¨äºæ‰‹åŠ¨æ·»åŠ è®ºæ–‡ä¸­çš„æ–°è®ºæ–‡
        filtered_new_papers = []
        for paper in new_papers:
            if not any(p['title'] == paper['title'] for p in existing_manual_papers):
                filtered_new_papers.append(paper)

        # åˆå¹¶æ‰€æœ‰è‡ªåŠ¨æ›´æ–°çš„è®ºæ–‡
        all_auto_papers = filtered_new_papers

        # æŒ‰æ—¥æœŸæ’åº
        existing_manual_papers.sort(key=lambda x: x['date'], reverse=True)
        all_auto_papers.sort(key=lambda x: x['date'], reverse=True)

        # å®šä¹‰å„ä¸»é¢˜çš„ç®€ä»‹å’Œä¸»è¦å†…å®¹
        category_intros = {
            "Motion-Planning": {
                "en": "This directory collects papers and code implementations related to motion planning in embodied AI.",
                "cn": "æœ¬ç›®å½•æ”¶é›†äº†å…·èº«æ™ºèƒ½ä¸­ä¸è¿åŠ¨è§„åˆ’ç›¸å…³çš„è®ºæ–‡å’Œä»£ç å®ç°ã€‚"
            },
            "Task-Planning": {
                "en": "This directory collects papers and code implementations related to task planning in embodied AI.",
                "cn": "æœ¬ç›®å½•æ”¶é›†äº†å…·èº«æ™ºèƒ½ä¸­ä¸ä»»åŠ¡è§„åˆ’ç›¸å…³çš„è®ºæ–‡å’Œä»£ç å®ç°ã€‚"
            },
            "Simulation-Platforms": {
                "en": "This directory collects papers and code implementations related to simulation platforms in embodied AI.",
                "cn": "æœ¬ç›®å½•æ”¶é›†äº†å…·èº«æ™ºèƒ½ä¸­ä¸ä»¿çœŸå¹³å°ç›¸å…³çš„è®ºæ–‡å’Œä»£ç å®ç°ã€‚"
            },
            "Robot-Learning-and-Reinforcement-Learning": {
                "en": "This directory collects papers and code implementations related to robot learning and reinforcement learning in embodied AI.",
                "cn": "æœ¬ç›®å½•æ”¶é›†äº†å…·èº«æ™ºèƒ½ä¸­ä¸æœºå™¨äººå­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ç›¸å…³çš„è®ºæ–‡å’Œä»£ç å®ç°ã€‚"
            },
            "Multimodal-Interaction": {
                "en": "This directory collects papers and code implementations related to multimodal interaction in embodied AI.",
                "cn": "æœ¬ç›®å½•æ”¶é›†äº†å…·èº«æ™ºèƒ½ä¸­ä¸å¤šæ¨¡æ€äº¤äº’ç›¸å…³çš„è®ºæ–‡å’Œä»£ç å®ç°ã€‚"
            },
            "Environment-Perception": {
                "en": "This directory collects papers and code implementations related to environment perception in embodied AI.",
                "cn": "æœ¬ç›®å½•æ”¶é›†äº†å…·èº«æ™ºèƒ½ä¸­ä¸ç¯å¢ƒæ„ŸçŸ¥ç›¸å…³çš„è®ºæ–‡å’Œä»£ç å®ç°ã€‚"
            },
            "Fundamental-Theory": {
                "en": "This directory collects papers and code implementations related to fundamental theory in embodied AI.",
                "cn": "æœ¬ç›®å½•æ”¶é›†äº†å…·èº«æ™ºèƒ½ä¸­ä¸åŸºç¡€ç†è®ºç›¸å…³çš„è®ºæ–‡å’Œä»£ç å®ç°ã€‚"
            }
        }

        # å®šä¹‰å„ä¸»é¢˜çš„ä¸»è¦å†…å®¹
        category_contents = {
            "Motion-Planning": {
                "en": [
                    "Dynamic Motion Planning",
                    "Whole-Body Motion Planning",
                    "Trajectory Optimization",
                    "Collision Avoidance",
                    "Real-time Planning",
                    "Humanoid Robot Control"
                ],
                "cn": [
                    "åŠ¨æ€è¿åŠ¨è§„åˆ’",
                    "å…¨èº«è¿åŠ¨è§„åˆ’",
                    "è½¨è¿¹ä¼˜åŒ–",
                    "ç¢°æ’é¿å…",
                    "å®æ—¶è§„åˆ’",
                    "äººå½¢æœºå™¨äººæ§åˆ¶"
                ]
            },
            "Task-Planning": {
                "en": [
                    "High-level Task Planning",
                    "Hierarchical Planning",
                    "Task and Motion Planning (TAMP)",
                    "Learning-based Planning",
                    "Multi-agent Planning"
                ],
                "cn": [
                    "é«˜å±‚ä»»åŠ¡è§„åˆ’",
                    "åˆ†å±‚è§„åˆ’",
                    "ä»»åŠ¡ä¸è¿åŠ¨è§„åˆ’(TAMP)",
                    "åŸºäºå­¦ä¹ çš„è§„åˆ’",
                    "å¤šæ™ºèƒ½ä½“è§„åˆ’"
                ]
            },
            "Simulation-Platforms": {
                "en": [
                    "Physics Simulators",
                    "Robot Simulation Environments",
                    "Learning Environments",
                    "Benchmarking Platforms",
                    "Digital Twins",
                    "Synthetic Data Generation"
                ],
                "cn": [
                    "ç‰©ç†ä»¿çœŸå™¨",
                    "æœºå™¨äººä»¿çœŸç¯å¢ƒ",
                    "å­¦ä¹ ç¯å¢ƒ",
                    "åŸºå‡†æµ‹è¯•å¹³å°",
                    "æ•°å­—å­ªç”Ÿ",
                    "åˆæˆæ•°æ®ç”Ÿæˆ"
                ]
            },
            "Robot-Learning-and-Reinforcement-Learning": {
                "en": [
                    "Foundational Works",
                    "Imitation Learning",
                    "Skill Learning",
                    "Multi-task and Transfer Learning",
                    "Advanced Policy Learning",
                    "Sim-to-Real Transfer",
                    "Cross-Morphology Learning"
                ],
                "cn": [
                    "å¥ åŸºæ€§å·¥ä½œ",
                    "æ¨¡ä»¿å­¦ä¹ ",
                    "æŠ€èƒ½å­¦ä¹ ",
                    "å¤šä»»åŠ¡ä¸è¿ç§»å­¦ä¹ ",
                    "é«˜çº§ç­–ç•¥å­¦ä¹ ",
                    "Sim-to-Realè¿ç§»",
                    "è·¨å½¢æ€å­¦ä¹ "
                ]
            },
            "Multimodal-Interaction": {
                "en": [
                    "Human-Robot Interaction",
                    "Natural Language Processing for Robots",
                    "Gesture Recognition and Understanding",
                    "Multi-modal Learning and Fusion",
                    "Social Robotics",
                    "Robot Reasoning and Memory"
                ],
                "cn": [
                    "äººæœºäº¤äº’",
                    "æœºå™¨äººè‡ªç„¶è¯­è¨€å¤„ç†",
                    "æ‰‹åŠ¿è¯†åˆ«ä¸ç†è§£",
                    "å¤šæ¨¡æ€å­¦ä¹ ä¸èåˆ",
                    "ç¤¾äº¤æœºå™¨äºº",
                    "æœºå™¨äººæ¨ç†ä¸è®°å¿†"
                ]
            },
            "Environment-Perception": {
                "en": [
                    "Visual Perception",
                    "Terrain Understanding",
                    "SLAM and Mapping",
                    "Scene Understanding",
                    "Sensor Fusion"
                ],
                "cn": [
                    "è§†è§‰æ„ŸçŸ¥",
                    "åœ°å½¢ç†è§£",
                    "SLAMä¸åœ°å›¾æ„å»º",
                    "åœºæ™¯ç†è§£",
                    "ä¼ æ„Ÿå™¨èåˆ"
                ]
            },
            "Fundamental-Theory": {
                "en": [
                    "Cognitive Foundations of Embodied Intelligence",
                    "Computational Models of Embodied Intelligence",
                    "Learning Theory in Embodied Intelligence",
                    "Evaluation Methods for Embodied Intelligence"
                ],
                "cn": [
                    "å…·èº«æ™ºèƒ½çš„è®¤çŸ¥åŸºç¡€",
                    "å…·èº«æ™ºèƒ½çš„è®¡ç®—æ¨¡å‹",
                    "å…·èº«æ™ºèƒ½çš„å­¦ä¹ ç†è®º",
                    "å…·èº«æ™ºèƒ½çš„è¯„ä¼°æ–¹æ³•"
                ]
            }
        }

        # ä¿®æ”¹ä»£ç é“¾æ¥æ ¼å¼
        def format_code_link(code_url):
            if code_url == "âš ï¸":
                return "âš ï¸"
            else:
                # æå–ä»£ç åç§°
                code_name = code_url.split("/")[-1].replace(")", "")
                # è¿”å›æ ‡å‡†çš„ Markdown é“¾æ¥æ ¼å¼
                return f"[{code_name}]({code_url})"

        # æ›´æ–°è‹±æ–‡README
        en_file = f"{category}/README.md"
        if os.path.exists(en_file):
            with open(en_file, "r", encoding="utf-8") as f:
                content = f.read()

                # æå–æ ‡é¢˜å’Œè¯­è¨€åˆ‡æ¢éƒ¨åˆ†
                title_match = re.match(r'(#.*?\n\n>.*?\n\n)', content, re.DOTALL)
                if title_match:
                    title_section = title_match.group(1)
                    en_content = f"{title_section}{category_intros[category]['en']}\n\n"
                    en_content += "## Main Contents\n\n"
                    for content in category_contents[category]['en']:
                        en_content += f"- {content}\n"
                    en_content += "\n"

                    # æ·»åŠ æ‰‹åŠ¨æ·»åŠ çš„è®ºæ–‡
                    en_content += "## Manually Added Papers\n\n"
                    if existing_manual_papers:
                        en_content += "|Date|Title|Paper|Code|Rating|\n"
                        en_content += "|:---:|:---:|:---:|:---:|:---:|\n"
                        for paper in existing_manual_papers:
                            code_link = format_code_link(paper['code_url'])
                            en_content += f"|{paper['date']}|{paper['title']}|[[pdf]]({paper['pdf_url']})|{code_link}|{paper['rating']}|\n"
                    en_content += "\n"

                    # æ·»åŠ è‡ªåŠ¨æ›´æ–°çš„è®ºæ–‡
                    en_content += "## Auto-Updated Papers\n\n"
                    if all_auto_papers:
                        en_content += "|Date|Title|Paper|Code|Rating|\n"
                        en_content += "|:---:|:---:|:---:|:---:|:---:|\n"
                        for paper in all_auto_papers:
                            code_link = format_code_link(paper['code_url'])
                            en_content += f"|{paper['date']}|{paper['title']}|[[pdf]]({paper['pdf_url']})|{code_link}|{paper['rating']}|\n"
                    en_content += "\n"

                    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                    total_papers = len(existing_manual_papers) + len(all_auto_papers)
                    code_implementations = sum(1 for p in existing_manual_papers + all_auto_papers if p['code_url'] != "âš ï¸")
                    en_content += "## ğŸ“Š Statistics\n\n"
                    en_content += f"- Total Papers: {total_papers}\n"
                    en_content += f"- Code Implementations: {code_implementations}\n"
                    en_content += f"- Last Updated: {datetime.now().strftime('%B %Y')}\n"

                    # å†™å…¥è‹±æ–‡README
                    with open(en_file, "w", encoding="utf-8") as f:
                        f.write(en_content)

        # æ›´æ–°ä¸­æ–‡README
        cn_file = f"{category}/README_CN.md"
        if os.path.exists(cn_file):
            with open(cn_file, "r", encoding="utf-8") as f:
                content = f.read()

                # æå–æ ‡é¢˜å’Œè¯­è¨€åˆ‡æ¢éƒ¨åˆ†
                title_match = re.match(r'(#.*?\n\n>.*?\n\n)', content, re.DOTALL)
                if title_match:
                    title_section = title_match.group(1)
                    cn_content = f"{title_section}{category_intros[category]['cn']}\n\n"
                    cn_content += "## ä¸»è¦å†…å®¹\n\n"
                    for content in category_contents[category]['cn']:
                        cn_content += f"- {content}\n"
                    cn_content += "\n"

                    # æ·»åŠ æ‰‹åŠ¨æ·»åŠ çš„è®ºæ–‡
                    cn_content += "## æ‰‹åŠ¨æ·»åŠ çš„è®ºæ–‡\n\n"
                    if existing_manual_papers:
                        cn_content += "|æ—¥æœŸ|æ ‡é¢˜|è®ºæ–‡|ä»£ç |æ¨èæŒ‡æ•°|\n"
                        cn_content += "|:---:|:---:|:---:|:---:|:---:|\n"
                        for paper in existing_manual_papers:
                            code_link = format_code_link(paper['code_url'])
                            cn_content += f"|{paper['date']}|{paper['title']}|[[pdf]]({paper['pdf_url']})|{code_link}|{paper['rating']}|\n"
                    cn_content += "\n"

                    # æ·»åŠ è‡ªåŠ¨æ›´æ–°çš„è®ºæ–‡
                    cn_content += "## è‡ªåŠ¨æ›´æ–°çš„è®ºæ–‡\n\n"
                    if all_auto_papers:
                        cn_content += "|æ—¥æœŸ|æ ‡é¢˜|è®ºæ–‡|ä»£ç |æ¨èæŒ‡æ•°|\n"
                        cn_content += "|:---:|:---:|:---:|:---:|:---:|\n"
                        for paper in all_auto_papers:
                            code_link = format_code_link(paper['code_url'])
                            cn_content += f"|{paper['date']}|{paper['title']}|[[pdf]]({paper['pdf_url']})|{code_link}|{paper['rating']}|\n"
                    cn_content += "\n"

                    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                    total_papers = len(existing_manual_papers) + len(all_auto_papers)
                    code_implementations = sum(1 for p in existing_manual_papers + all_auto_papers if p['code_url'] != "âš ï¸")
                    cn_content += "## ğŸ“Š ç»Ÿè®¡\n\n"
                    cn_content += f"- è®ºæ–‡æ€»æ•°ï¼š{total_papers}ç¯‡\n"
                    cn_content += f"- ä»£ç å®ç°ï¼š{code_implementations}ä¸ª\n"
                    cn_content += f"- æœ€åæ›´æ–°ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ')}\n"

                    # å†™å…¥ä¸­æ–‡README
                    with open(cn_file, "w", encoding="utf-8") as f:
                        f.write(cn_content)

        # æ›´æ–°ç°æœ‰è®ºæ–‡åˆ—è¡¨
        self.existing_papers[category] = existing_manual_papers + all_auto_papers

    def update_root_readme(self):
        """æ›´æ–°æ ¹ç›®å½•çš„READMEæ–‡ä»¶"""
        total_papers = 0
        total_codes = 0

        # ç»Ÿè®¡æ‰€æœ‰åˆ†ç±»çš„è®ºæ–‡å’Œä»£ç æ•°é‡
        for category in self.categories.keys():
            if os.path.exists(f"{category}/README.md"):
                with open(f"{category}/README.md", "r", encoding="utf-8") as f:
                    content = f.read()
                    # è®¡ç®—è®ºæ–‡æ•°é‡ï¼ˆå‡å»è¡¨å¤´ï¼‰
                    papers = len(re.findall(r'\|.*?\|.*?\|.*?\|.*?\|.*?\|', content)) - 1
                    # è®¡ç®—æœ‰ä»£ç å®ç°çš„è®ºæ–‡æ•°é‡
                    codes = len(re.findall(r'\[\[(?!âš ï¸).*?\]\]', content))
                    total_papers += papers
                    total_codes += codes

        # è¯»å–ç°æœ‰çš„è‹±æ–‡READMEå†…å®¹
        with open("README.md", "r", encoding="utf-8") as f:
            content = f.read()

        # åªæ›´æ–°ç»Ÿè®¡ä¿¡æ¯éƒ¨åˆ†
        new_content = re.sub(
            r'## Statistics\n\n- Total Papers: \d+\n- Code Implementations: \d+',
            f'## Statistics\n\n- Total Papers: {total_papers}\n- Code Implementations: {total_codes}',
            content
        )

        # å†™å…¥æ›´æ–°åçš„å†…å®¹
        with open("README.md", "w", encoding="utf-8") as f:
            f.write(new_content)

        # è¯»å–ç°æœ‰çš„ä¸­æ–‡READMEå†…å®¹
        with open("README_CN.md", "r", encoding="utf-8") as f:
            content = f.read()

        # åªæ›´æ–°ç»Ÿè®¡ä¿¡æ¯éƒ¨åˆ†
        new_content = re.sub(
            r'## ğŸ“Šç»Ÿè®¡\n\n- è®ºæ–‡æ€»æ•°ï¼š\d+ç¯‡\n- ä»£ç å®ç°ï¼š\d+ä¸ª',
            f'## ğŸ“Šç»Ÿè®¡\n\n- è®ºæ–‡æ€»æ•°ï¼š{total_papers}ç¯‡\n- ä»£ç å®ç°ï¼š{total_codes}ä¸ª',
            content
        )

        # å†™å…¥æ›´æ–°åçš„å†…å®¹
        with open("README_CN.md", "w", encoding="utf-8") as f:
            f.write(new_content)

    def run(self):
        """è¿è¡Œæ›´æ–°ç¨‹åº"""
        print("å¼€å§‹æ›´æ–°è®ºæ–‡åˆ—è¡¨...")

        for category, query in self.keywords.items():
            print(f"\nå¤„ç†åˆ†ç±»: {category}")
            print(f"ä½¿ç”¨æŸ¥è¯¢: {query}")
            papers = self.get_daily_papers(category, query)
            if papers:
                print(f"æ‰¾åˆ° {len(papers)} ç¯‡æ–°è®ºæ–‡")
                self.update_category_readme(category, papers)
            else:
                print("æ²¡æœ‰æ‰¾åˆ°æ–°è®ºæ–‡")

        self.update_root_readme()
        print("\næ›´æ–°å®Œæˆï¼")

if __name__ == "__main__":
    updater = PaperUpdater()
    updater.run()