# Embodied AI Research Resources

<div align='center'>
  <img src="assets/logo.svg" width=250px >
</div>

<div align='center'>
  <img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" >
  <img src="https://img.shields.io/badge/License-MIT-turquoise.svg" >
  <img src="https://img.shields.io/badge/PRs-Welcome-brightgreen.svg" >
  <a href="README_CN.md">ä¸­æ–‡</a>
</div>

> ğŸŒ Language: [English](README.md) | [ä¸­æ–‡](README_CN.md)

This repository collects research resources in the field of Embodied AI, including papers, code implementations, and datasets. Embodied AI studies how robots can learn and accomplish tasks through interaction with the environment, which is an important intersection of artificial intelligence and robotics.

## Domain Relationships

Embodied AI research involves multiple interconnected domains:

- **Fundamental Theory**: Provides theoretical foundations for embodied AI, including cognitive science and control theory
- **Robot Learning & Reinforcement Learning**: Enables autonomous learning and decision-making capabilities for robots
- **Environment Perception**: Enables robots to understand and adapt to their environment, including visual language understanding
- **Motion Planning & Task Planning**: Implements robot action execution and task decomposition
- **Multimodal Interaction**: Supports multimodal interaction between robots and environment/humans
- **Simulation Platforms**: Provides virtual environments for robot learning and testing

## ğŸ“– Contents

* [Fundamental Theory](Fundamental-Theory/README.md)
* [Robot Learning & Reinforcement Learning](Robot-Learning-and-Reinforcement-Learning/README.md)
* [Environment Perception](Environment-Perception/README.md)
* [Motion Planning](Motion-Planning/README.md)
* [Task Planning](Task-Planning/README.md)
* [Multimodal Interaction](Multimodal-Interaction/README.md)
* [Simulation Platforms](Simulation-Platforms/README.md)

## Statistics

- Total Papers: 480
- Code Implementations: 462

## Citation

If you use this repository in your research, please cite:

```BibTeX
@misc{Awesome-Embodied-AI@2025,
  title={Awesome-Embodied-AI: A curated list of Awesome Embodied AI Papers with codes},
  url={https://github.com/GlimmerLab/Awesome-Embodied-AI},
  author={GlimmerLab etc},
  year={2025}
}
```

## Contributing

We welcome new research resources! Please ensure:

1. Resources have high academic or practical value
2. Complete paper links and code repositories are provided (if available)
3. Resources are added to appropriate directories following the existing format
4. Statistics are updated accordingly

## License

This project is licensed under the [MIT License](LICENSE).

## ğŸ¯Features

- ğŸ“š Comprehensive paper collection
- ğŸ’» Open source implementations
- ğŸ” Detailed categorization
- ğŸŒŸ Important paper recommendations
- ğŸ“… Regular updates

## ğŸŒŸAcknowledgments

Thanks to all researchers and developers who have contributed to this project!

## ğŸ“¬Contact

- Project Maintainer: [GlimmerLab](junli440883@gmail.com)
- Project Homepage: [GitHub](https://github.com/GlimmerLab/Awesome-Embodied-AI)
<!-- - Twitter: [@GlimmerLab](https://twitter.com/GlimmerLab) -->
<!-- - Discord: [Join our community](https://discord.gg/glimmerlab) -->

## ğŸŒŸContributors

Thanks to all our contributors:

<a href="https://github.com/GlimmerLab/Awesome-Embodied-AI/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=GlimmerLab/Awesome-Embodied-AI" />
</a>

Made with [contrib.rocks](https://contrib.rocks).

---

â­ï¸ If you find this project helpful, please star it!